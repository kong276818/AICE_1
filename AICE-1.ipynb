{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3503128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_4000TT = pd.read_csv('4000TT.csv')\n",
    "df_input = pd.read_csv('input.csv')\n",
    "\n",
    "# 2. 'machine_name'ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
    "merged_df = pd.merge(df_4000TT, df_input, on='machine_name')\n",
    "\n",
    "# 3. 'input' ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ infoDFë¡œ ì €ì¥\n",
    "infoDF = merged_df[['input']]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(infoDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì˜ˆì‹œ: infoDFì— Address ì—´ì´ ìˆë‹¤ê³  ê°€ì •\n",
    "# infoDF = pd.read_csv(\"your_file.csv\")  # í•„ìš”ì‹œ íŒŒì¼ ë¡œë”©\n",
    "\n",
    "# 1. Address ì—´ì—ì„œ í–‰ì •ë™ ì¶”ì¶œ (ë³´í†µ ë§ˆì§€ë§‰ ë‹¨ì–´)\n",
    "infoDF['í–‰ì •ë™'] = infoDF['Address'].str.split().str[-1]\n",
    "\n",
    "# 2. í–‰ì •ë™ë³„ ë¹ˆë„ í™•ì¸\n",
    "print(infoDF['í–‰ì •ë™'].value_counts())\n",
    "\n",
    "# 3. ì‹œê°í™” - countplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=infoDF, x='í–‰ì •ë™', order=infoDF['í–‰ì •ë™'].value_counts().index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('í–‰ì •ë™')\n",
    "plt.ylabel('ë¹ˆë„ìˆ˜')\n",
    "plt.title('í–‰ì •ë™ ë¶„í¬')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Time_Driving (ì£¼í–‰ì‹œê°„) ë¶„í¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=infoDF, x='Time_Driving', kde=True)\n",
    "plt.title('ì£¼í–‰ì‹œê°„(Time_Driving) ë¶„í¬')\n",
    "plt.xlabel('Time_Driving')\n",
    "plt.ylabel('ë¹ˆë„ìˆ˜')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Speed_Per_Hour (í‰ê· ì†ë„) ë¶„í¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=infoDF, x='Speed_Per_Hour', kde=True)\n",
    "plt.title('í‰ê· ì†ë„(Speed_Per_Hour) ë¶„í¬')\n",
    "plt.xlabel('Speed_Per_Hour')\n",
    "plt.ylabel('ë¹ˆë„ìˆ˜')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ìƒì¹˜ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "print(\"300 ì´ìƒ ì†ë„ ë°ì´í„° ìˆ˜:\", (infoDF['Speed_Per_Hour'] >= 300).sum())\n",
    "\n",
    "# ì´ìƒì¹˜ ì œê±°\n",
    "infoDF = infoDF[infoDF['Speed_Per_Hour'] < 300]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜:\", len(infoDF))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 300 ì´ìƒì¸ ê°’ì€ 300ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "infoDF['Speed_Per_Hour'] = np.where(infoDF['Speed_Per_Hour'] >= 300, 300, infoDF['Speed_Per_Hour'])\n",
    "\n",
    "# í™•ì¸\n",
    "print(infoDF['Speed_Per_Hour'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903af4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "infoDF = infoDF[\n",
    "    (infoDF['Speed_Per_Hour'] >= 10) & (infoDF['Speed_Per_Hour'] <= 300) &\n",
    "    (infoDF['Time_Driving'] >= 1) & (infoDF['Time_Driving'] <= 1000)\n",
    "]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"ì¡°ê±´ ì ìš© í›„ ë°ì´í„° ìˆ˜:\", len(infoDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# ì •ê·œí™” ìˆ˜í–‰ ë° ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€\n",
    "infoDF[['Speed_Scaled', 'Time_Scaled']] = scaler.fit_transform(infoDF[['Speed_Per_Hour', 'Time_Driving']])\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(infoDF[['Speed_Per_Hour', 'Speed_Scaled', 'Time_Driving', 'Time_Scaled']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68529f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Address ì»¬ëŸ¼ì—ì„œ í–‰ì •ë™ ì¶”ì¶œ (ë³´í†µ ë§ˆì§€ë§‰ ë‹¨ì–´)\n",
    "infoDF['í–‰ì •ë™'] = infoDF['Address'].str.split().str[-1]\n",
    "\n",
    "# 2. í–‰ì •ë™ ì»¬ëŸ¼ì— ëŒ€í•´ ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰\n",
    "one_hot_df = pd.get_dummies(infoDF['í–‰ì •ë™'], prefix='ë™')\n",
    "\n",
    "# 3. infoDFì— ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ë¶™ì´ê¸°\n",
    "infoDF = pd.concat([infoDF, one_hot_df], axis=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(infoDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. X, y ì •ì˜\n",
    "X = infoDF.drop(columns=['Time_Driving'])  # íŠ¹ì„±\n",
    "y = infoDF['Time_Driving']                # íƒ€ê²Ÿ\n",
    "\n",
    "# 2. í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (80:20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. StandardScaler ì ìš©\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬ (ì›ë˜ ì»¬ëŸ¼ëª… ìœ ì§€)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "# 5. ë°ì´í„° ë³‘í•© (ìŠ¤ì¼€ì¼ëœ Xì™€ ì›ë˜ y)\n",
    "DataScaled_train = pd.concat([X_train_scaled_df, y_train], axis=1)\n",
    "DataScaled_test = pd.concat([X_test_scaled_df, y_test], axis=1)\n",
    "\n",
    "# ì„ íƒì ìœ¼ë¡œ ì „ì²´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„°ë¥¼ í•©ì³ í•˜ë‚˜ì˜ DataScaledë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŒ\n",
    "DataScaled = pd.concat([DataScaled_train, DataScaled_test])\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(DataScaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y) ë¶„ë¦¬\n",
    "X = infoDF.drop(columns=['Time_Driving'])\n",
    "y = infoDF['Time_Driving']\n",
    "\n",
    "# 2. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. ì˜ˆì¸¡\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# 5. ì„±ëŠ¥ í‰ê°€ (RMSE)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, dt_pred))\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "\n",
    "print(f\"Decision Tree RMSE: {dt_rmse:.2f}\")\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.2f}\")\n",
    "\n",
    "# 6. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ì˜ì‚¬ê²°ì •ë‚˜ë¬´\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, dt_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Time_Driving\")\n",
    "plt.ylabel(\"Predicted (Decision Tree)\")\n",
    "plt.title(\"Decision Tree Prediction vs Actual\")\n",
    "\n",
    "# ëœë¤í¬ë ˆìŠ¤íŠ¸\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, rf_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Time_Driving\")\n",
    "plt.ylabel(\"Predicted (Random Forest)\")\n",
    "plt.title(\"Random Forest Prediction vs Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. MAE ê³„ì‚°\n",
    "dt_mae = mean_absolute_error(y_test, dt_pred)\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "\n",
    "# 2. ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"Decision Tree MAE: {dt_mae:.2f}\")\n",
    "print(f\"Random Forest MAE: {rf_mae:.2f}\")\n",
    "\n",
    "# 3. ê°„ë‹¨í•œ í•´ì„\n",
    "if dt_mae < rf_mae:\n",
    "    print(\"ğŸ‘‰ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ì˜ MAEê°€ ë” ë‚®ìœ¼ë¯€ë¡œ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\")\n",
    "elif rf_mae < dt_mae:\n",
    "    print(\"ğŸ‘‰ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ MAEê°€ ë” ë‚®ìœ¼ë¯€ë¡œ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ğŸ‘‰ ë‘ ëª¨ë¸ì˜ MAEê°€ ë™ì¼í•˜ê±°ë‚˜ ê±°ì˜ ë¹„ìŠ·í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ab75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),  # ì…ë ¥ì¸µ + ì€ë‹‰ì¸µ1\n",
    "    Dense(32, activation='relu'),                                    # ì€ë‹‰ì¸µ2\n",
    "    Dense(1)                                                         # ì¶œë ¥ì¸µ (íšŒê·€ ë¬¸ì œ)\n",
    "])\n",
    "\n",
    "# 2. ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(\n",
    "    loss='mae',                        # ì†ì‹¤ í•¨ìˆ˜: MAE\n",
    "    optimizer='adam',                 # ìµœì í™” ë„êµ¬: Adam\n",
    "    metrics=['mae', 'mse']            # í‰ê°€ì§€í‘œ: MAE, MSE\n",
    ")\n",
    "\n",
    "# 3. í•™ìŠµ ìˆ˜í–‰\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ì†ì‹¤(Loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss (MAE)')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss (MAE)')\n",
    "plt.title('Loss (MAE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('Mean Absolute Error (MAE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb335436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MSE ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['mse'], label='Train MSE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
    "plt.title('Model MSE Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
